{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Construção de um Classificador Binário baseado no algoritmo KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependências necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[50]:\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import pandas as pd\n",
    "import nltk  \n",
    "import numpy as np  \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import heapq\n",
    "import re  \n",
    "import io\n",
    "import math\n",
    "import csv\n",
    "# Helper libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import functools\n",
    "import operator\n",
    "import PIL\n",
    "import tqdm\n",
    "import tqdm.auto\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando variáveis de ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV_DATASET_FILENAME: Nome do arquivo que contém o dataset: locations_to_be_labeled.csv\n",
      "ORDER_DEGREE: As 400 palavras mais frequentes da bag of words serão consideradas na construção do modelo.\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# ENV_DATASET_FILENAME = os.environ['INPUT_DATASET_FILENAME']\n",
    "# ENV_ORDER_DEGREE = os.environ['ORDER_DEGREE']\n",
    "\n",
    "ENV_DATASET_FILENAME = \"locations_to_be_labeled.csv\"\n",
    "ENV_ORDER_DEGREE = 400\n",
    "\n",
    "nameOfTheFile = ENV_DATASET_FILENAME\n",
    "print(\"ENV_DATASET_FILENAME: Nome do arquivo que contém o dataset: \" + nameOfTheFile)\n",
    "print(\"ORDER_DEGREE: As \" + str(ENV_ORDER_DEGREE) + \" palavras mais frequentes da bag of words serão consideradas na construção do modelo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação do conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de perguntas: 1500\n",
      "\n",
      "Tweet:\n",
      "\n",
      "- com o cara faixapreta que venceu a dengue ducatambasco haha foi muito bom te rever brother\n",
      "- dengue me pegou dnv\n",
      "- prefeitura de santa cruz divulga plano de ação de enfrentamento a dengue chicungunya e zika\n",
      "- seguimos na luta contra a dengue chikungunya e o zika vírus com essas três doenças não dá para\n",
      "- g1 pr segunda morte por dengue em paranaguá é confirmada pela saúde  g1 \n",
      "...\n",
      "\n",
      "\n",
      "Respostas:\n",
      "\n",
      "- 1\n",
      "- 1\n",
      "- 0\n",
      "- 0\n",
      "- 1\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Lendo as features\n",
    "questions = pd.read_csv(\"../\" + nameOfTheFile, header=0, usecols=[0])\n",
    "listOfQuestions = []\n",
    "for row in questions.values:\n",
    "    listOfQuestions.append(list(row)[0])\n",
    "\n",
    "print(\"Quantidade de perguntas: \" + str(len(listOfQuestions)) + \"\\n\")\n",
    "print(\"Tweet:\\n\")\n",
    "for pergunta in listOfQuestions[0:5]:\n",
    "    print(\"- \" + pergunta)\n",
    "print(\"...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Lendo as labels\n",
    "answers = pd.read_csv(\"../\" + nameOfTheFile, header=0, usecols=[1])\n",
    "listOfAnswers = []\n",
    "for answer in answers.values:\n",
    "    listOfAnswers.append(list(answer)[0])\n",
    "print(\"Respostas:\\n\")\n",
    "for label in listOfAnswers[0:5]:\n",
    "    print(\"- \" + str(label))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando as labels\n",
    "\n",
    "Essa transformação vai trocar as labels de número para texto, com o objetivo de facilitar a compreensão e leitura de resultados.\n",
    "0 - generico\n",
    "1 - doente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doente', 'doente', 'generico', 'generico', 'doente']\n"
     ]
    }
   ],
   "source": [
    "label_palavras = []\n",
    "for item in listOfAnswers:\n",
    "    if item==0:\n",
    "        label_palavras.append(\"generico\")\n",
    "    else:\n",
    "        label_palavras.append(\"doente\")\n",
    "print(label_palavras[0:5])\n",
    "listOfAnswers = label_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de espaçamentos extras, caracteres especiais e pontuações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- com o cara faixapreta que venceu a dengue ducatambasco haha foi muito bom te rever brother\n",
      "- dengue me pegou dnv\n",
      "- prefeitura de santa cruz divulga plano de ação de enfrentamento a dengue chicungunya e zika\n",
      "- seguimos na luta contra a dengue chikungunya e o zika vírus com essas três doenças não dá para\n",
      "- g1 pr segunda morte por dengue em paranaguá é confirmada pela saúde g1 \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(listOfQuestions)):\n",
    "    listOfQuestions [i] = listOfQuestions [i].lower()\n",
    "    listOfQuestions [i] = re.sub(r'\\W',' ',listOfQuestions [i])\n",
    "    listOfQuestions [i] = re.sub(r'\\s+',' ',listOfQuestions [i])\n",
    "\n",
    "for question in listOfQuestions[0:5]:\n",
    "    print(\"- \" + question)\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização dos tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ['com', 'o', 'cara', 'faixapreta', 'que', 'venceu', 'a', 'dengue', 'ducatambasco', 'haha', 'foi', 'muito', 'bom', 'te', 'rever', 'brother']\n",
      "- ['dengue', 'me', 'pegou', 'dnv']\n",
      "- ['prefeitura', 'de', 'santa', 'cruz', 'divulga', 'plano', 'de', 'ação', 'de', 'enfrentamento', 'a', 'dengue', 'chicungunya', 'e', 'zika']\n",
      "- ['seguimos', 'na', 'luta', 'contra', 'a', 'dengue', 'chikungunya', 'e', 'o', 'zika', 'vírus', 'com', 'essas', 'três', 'doenças', 'não', 'dá', 'para']\n",
      "- ['g1', 'pr', 'segunda', 'morte', 'por', 'dengue', 'em', 'paranaguá', 'é', 'confirmada', 'pela', 'saúde', 'g1']\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentences(listOfSentences):\n",
    "    listOfSentencesInTokens = []\n",
    "    for i in range(len(listOfSentences)):\n",
    "        tokens = nltk.word_tokenize(listOfSentences[i])\n",
    "        listOfSentencesInTokens.append(tokens)\n",
    "    return listOfSentencesInTokens\n",
    "\n",
    "\n",
    "listOfTokenizedQuestions = tokenize_sentences(listOfQuestions)\n",
    "for tokenizedItem in listOfTokenizedQuestions[0:5]:\n",
    "      print(\"- \" + str(tokenizedItem))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção das stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho das stop-words: 204\n",
      "Tamanho: 1500\n",
      "- ['cara', 'faixapreta', 'venceu', 'dengue', 'ducatambasco', 'haha', 'bom', 'rever', 'brother']\n",
      "- ['dengue', 'pegou', 'dnv']\n",
      "- ['prefeitura', 'santa', 'cruz', 'divulga', 'plano', 'ação', 'enfrentamento', 'dengue', 'chicungunya', 'zika']\n",
      "- ['seguimos', 'luta', 'contra', 'dengue', 'chikungunya', 'zika', 'vírus', 'três', 'doenças', 'dá']\n",
      "- ['g1', 'pr', 'segunda', 'morte', 'dengue', 'paranaguá', 'confirmada', 'saúde', 'g1']\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords[:10]\n",
    "print(\"Tamanho das stop-words: \" + str(len(stopwords)))\n",
    "\n",
    "def remove_stop_words(listOfTokenizedSentences):\n",
    "    sentencesWithNoStopWords = []\n",
    "    for i in range(len(listOfTokenizedSentences)):\n",
    "        sentenceWithNoStopWord = []\n",
    "        for j in range(len(listOfTokenizedSentences[i])):\n",
    "            if(listOfTokenizedSentences[i][j] not in stopwords):\n",
    "                sentenceWithNoStopWord.append(listOfTokenizedSentences[i][j])\n",
    "        sentencesWithNoStopWords.append(sentenceWithNoStopWord)\n",
    "    return sentencesWithNoStopWords\n",
    "\n",
    "listOfQuestionsWithNoStopWords = remove_stop_words(listOfTokenizedQuestions)\n",
    "print(\"Tamanho: \" + str(len(listOfQuestionsWithNoStopWords)))\n",
    "\n",
    "for itemWithNoStopWord in listOfQuestionsWithNoStopWords[0:5]:\n",
    "    print(\"- \" + str(itemWithNoStopWord))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematização das tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentenças lematizadas e filtradas: \n",
      "Tamanho: 1442\n",
      "- ['car', 'faixapret', 'venc', 'deng', 'ducatambasc', 'hah', 'bom', 'rev', 'broth']\n",
      "- ['deng', 'peg', 'dnv']\n",
      "- ['prefeitur', 'sant', 'cruz', 'divulg', 'plan', 'açã', 'enfrent', 'deng', 'chicunguny', 'zik']\n",
      "- ['segu', 'lut', 'contr', 'deng', 'chikunguny', 'zik', 'vírus', 'três', 'doenc', 'dá']\n",
      "- ['g1', 'pr', 'segund', 'mort', 'deng', 'paranagu', 'confirm', 'saúd', 'g1']\n",
      "...\n",
      "\n",
      "\n",
      "Labels lematizadas e filtradas: \n",
      "Tamanho: 1442\n",
      "doente\n",
      "doente\n",
      "generico\n",
      "generico\n",
      "doente\n",
      "...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"portuguese\")\n",
    "print()\n",
    "\n",
    "def lematizar_tokens(listOfTokenizedSentences):\n",
    "    stemmed_sentences = []\n",
    "    for tokenizedSentence in listOfTokenizedSentences:\n",
    "        stemmed_sentence = []\n",
    "        for token in tokenizedSentence:\n",
    "            stemmed_sentence.append(stemmer.stem(token))\n",
    "        stemmed_sentences.append(stemmed_sentence)\n",
    "    return stemmed_sentences\n",
    "\n",
    "def remove_redundancies(stemmed_sentences, labels):\n",
    "    filtered_list = []\n",
    "    filtered_labels = []\n",
    "    for index in range(len(stemmed_sentences)):\n",
    "        isRedundant = False\n",
    "        for filtered_sentence in filtered_list:\n",
    "            if(filtered_sentence == stemmed_sentences[index]):\n",
    "                isRedundant = True\n",
    "        if(not isRedundant):\n",
    "            filtered_list.append(stemmed_sentences[index])\n",
    "            filtered_labels.append(labels[index])\n",
    "    return {\"features\": filtered_list, \"labels\": filtered_labels}\n",
    "\n",
    "sentencas_lematizadas = lematizar_tokens(listOfQuestionsWithNoStopWords)\n",
    "filtered_dataset = remove_redundancies(sentencas_lematizadas, listOfAnswers)\n",
    "\n",
    "sentencas_lematizadas_e_filtradas = filtered_dataset[\"features\"]\n",
    "labels_lematizadas_e_filtradas = filtered_dataset[\"labels\"]\n",
    "\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(\"Sentenças lematizadas: \")\n",
    "# print(sentencas_lematizadas)\n",
    "# print(\"Tamanho: \" + str(len(sentencas_lematizadas)))\n",
    "# print(\"\\n\")\n",
    "\n",
    "print(\"Sentenças lematizadas e filtradas: \")\n",
    "print(\"Tamanho: \" + str(len(sentencas_lematizadas_e_filtradas)))\n",
    "for stemmedItem in sentencas_lematizadas_e_filtradas[0:5]:\n",
    "    print(\"- \" + str(stemmedItem))\n",
    "print(\"...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Labels lematizadas e filtradas: \")\n",
    "print(\"Tamanho: \" + str(len(labels_lematizadas_e_filtradas)))\n",
    "for stemmedLabel in labels_lematizadas_e_filtradas[0:5]:\n",
    "    print(stemmedLabel)\n",
    "print(\"...\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção da bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da bag of words: 3246\n"
     ]
    }
   ],
   "source": [
    "def create_bag_of_words(listOfTokenizedSentences):\n",
    "    wordfreq = {}\n",
    "    for i in range(len(listOfTokenizedSentences)):  #Para cada sentença tokenizada\n",
    "        for token in listOfTokenizedSentences[i]:       # Para cada token em uma sentença tokenizada\n",
    "            if token not in wordfreq.keys():\n",
    "                wordfreq[token] = 1\n",
    "            else:\n",
    "                wordfreq[token] += 1\n",
    "    return wordfreq\n",
    "\n",
    "wordfreq = create_bag_of_words(sentencas_lematizadas_e_filtradas)\n",
    "# print(wordfreq)\n",
    "print(\"Tamanho da bag of words: \" + str(len(wordfreq.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo do índice IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ENV_ORDER_DEGREE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ed4dcd2aeeec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morder_degree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENV_ORDER_DEGREE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_IDF_wordDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentencas_lematizadas_e_filtradas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmost_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mword_idf_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msentences_containing_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ENV_ORDER_DEGREE' is not defined"
     ]
    }
   ],
   "source": [
    "order_degree = ENV_ORDER_DEGREE\n",
    "def get_IDF_wordDictionary(sentencas_lematizadas_e_filtradas, most_freq):\n",
    "    word_idf_values = {}\n",
    "    for token in most_freq:\n",
    "        sentences_containing_word = 0\n",
    "        for tokenized_sentence in sentencas_lematizadas_e_filtradas:\n",
    "            if token in tokenized_sentence:\n",
    "                sentences_containing_word += 1\n",
    "        word_idf_values[token] = np.log(len(sentencas_lematizadas_e_filtradas)/(1 + sentences_containing_word))\n",
    "    return word_idf_values\n",
    "\n",
    "most_freq_dictionary = heapq.nlargest(order_degree , wordfreq, key=wordfreq.get)\n",
    "# print(\"Palavras mais frequentes e seus respectivos IDFs: \")\n",
    "idf_wordDictionary = get_IDF_wordDictionary(sentencas_lematizadas_e_filtradas, most_freq_dictionary)\n",
    "# print(idf_wordDictionary)\n",
    "# for key in list(idf_wordDictionary.keys())[0:5]:\n",
    "#   print(key + \": \" + str(idf_wordDictionary[key]))\n",
    "# print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
