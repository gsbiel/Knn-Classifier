{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Construção de um Classificador Binário baseado no algoritmo KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependências necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# In[50]:\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import pandas as pd\n",
    "import nltk  \n",
    "import numpy as np  \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import heapq\n",
    "import re  \n",
    "import io\n",
    "import math\n",
    "import csv\n",
    "# Helper libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import functools\n",
    "import operator\n",
    "import PIL\n",
    "import tqdm\n",
    "import tqdm.auto\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from IPython.display import display\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando variáveis de ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENV_DATASET_FILENAME: Nome do arquivo que contém o dataset: locations_to_be_labeled.csv\n",
      "ORDER_DEGREE: As 3000 palavras mais frequentes da bag of words serão consideradas na construção do modelo.\n",
      "ENV_TRAIN_TEST_PROPORTION: proporção da divisão do dataset em treino e teste: 90.0%\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# ENV_DATASET_FILENAME = os.environ['INPUT_DATASET_FILENAME']\n",
    "# ENV_ORDER_DEGREE = os.environ['ORDER_DEGREE']\n",
    "# ENV_TRAIN_TEST_PROPORTION = os.environ['TRAIN_TEST_PROPORTION']\n",
    "\n",
    "ENV_DATASET_FILENAME = \"locations_to_be_labeled.csv\"\n",
    "ENV_ORDER_DEGREE = 3000\n",
    "ENV_TRAIN_TEST_PROPORTION = 0.9\n",
    "\n",
    "nameOfTheFile = ENV_DATASET_FILENAME\n",
    "print(\"ENV_DATASET_FILENAME: Nome do arquivo que contém o dataset: \" + nameOfTheFile)\n",
    "print(\"ORDER_DEGREE: As \" + str(ENV_ORDER_DEGREE) + \" palavras mais frequentes da bag of words serão consideradas na construção do modelo.\")\n",
    "print(\"ENV_TRAIN_TEST_PROPORTION: proporção da divisão do dataset em treino e teste: \" + str(ENV_TRAIN_TEST_PROPORTION*100) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparação do conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de perguntas: 1500\n",
      "\n",
      "Tweet:\n",
      "\n",
      "- com o cara faixapreta que venceu a dengue ducatambasco haha foi muito bom te rever brother\n",
      "- dengue me pegou dnv\n",
      "- prefeitura de santa cruz divulga plano de ação de enfrentamento a dengue chicungunya e zika\n",
      "- seguimos na luta contra a dengue chikungunya e o zika vírus com essas três doenças não dá para\n",
      "- g1 pr segunda morte por dengue em paranaguá é confirmada pela saúde  g1 \n",
      "...\n",
      "\n",
      "\n",
      "Respostas:\n",
      "\n",
      "- 1\n",
      "- 1\n",
      "- 0\n",
      "- 0\n",
      "- 1\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Lendo as features\n",
    "questions = pd.read_csv(\"../\" + nameOfTheFile, header=0, usecols=[0])\n",
    "listOfQuestions = []\n",
    "for row in questions.values:\n",
    "    listOfQuestions.append(list(row)[0])\n",
    "\n",
    "print(\"Quantidade de perguntas: \" + str(len(listOfQuestions)) + \"\\n\")\n",
    "print(\"Tweet:\\n\")\n",
    "for pergunta in listOfQuestions[0:5]:\n",
    "    print(\"- \" + pergunta)\n",
    "print(\"...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# Lendo as labels\n",
    "answers = pd.read_csv(\"../\" + nameOfTheFile, header=0, usecols=[1])\n",
    "listOfAnswers = []\n",
    "for answer in answers.values:\n",
    "    listOfAnswers.append(list(answer)[0])\n",
    "print(\"Respostas:\\n\")\n",
    "for label in listOfAnswers[0:5]:\n",
    "    print(\"- \" + str(label))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformando as labels\n",
    "\n",
    "Essa transformação vai trocar as labels de número para texto, com o objetivo de facilitar a compreensão e leitura de resultados.\n",
    "0 - generico\n",
    "1 - doente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doente', 'doente', 'generico', 'generico', 'doente']\n"
     ]
    }
   ],
   "source": [
    "label_palavras = []\n",
    "for item in listOfAnswers:\n",
    "    if item==0:\n",
    "        label_palavras.append(\"generico\")\n",
    "    else:\n",
    "        label_palavras.append(\"doente\")\n",
    "print(label_palavras[0:5])\n",
    "listOfAnswers = label_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de espaçamentos extras, caracteres especiais e pontuações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- com o cara faixapreta que venceu a dengue ducatambasco haha foi muito bom te rever brother\n",
      "- dengue me pegou dnv\n",
      "- prefeitura de santa cruz divulga plano de ação de enfrentamento a dengue chicungunya e zika\n",
      "- seguimos na luta contra a dengue chikungunya e o zika vírus com essas três doenças não dá para\n",
      "- g1 pr segunda morte por dengue em paranaguá é confirmada pela saúde g1 \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(listOfQuestions)):\n",
    "    listOfQuestions [i] = listOfQuestions [i].lower()\n",
    "    listOfQuestions [i] = re.sub(r'\\W',' ',listOfQuestions [i])\n",
    "    listOfQuestions [i] = re.sub(r'\\s+',' ',listOfQuestions [i])\n",
    "\n",
    "for question in listOfQuestions[0:5]:\n",
    "    print(\"- \" + question)\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização dos tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- ['com', 'o', 'cara', 'faixapreta', 'que', 'venceu', 'a', 'dengue', 'ducatambasco', 'haha', 'foi', 'muito', 'bom', 'te', 'rever', 'brother']\n",
      "- ['dengue', 'me', 'pegou', 'dnv']\n",
      "- ['prefeitura', 'de', 'santa', 'cruz', 'divulga', 'plano', 'de', 'ação', 'de', 'enfrentamento', 'a', 'dengue', 'chicungunya', 'e', 'zika']\n",
      "- ['seguimos', 'na', 'luta', 'contra', 'a', 'dengue', 'chikungunya', 'e', 'o', 'zika', 'vírus', 'com', 'essas', 'três', 'doenças', 'não', 'dá', 'para']\n",
      "- ['g1', 'pr', 'segunda', 'morte', 'por', 'dengue', 'em', 'paranaguá', 'é', 'confirmada', 'pela', 'saúde', 'g1']\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def tokenize_sentences(listOfSentences):\n",
    "    listOfSentencesInTokens = []\n",
    "    for i in range(len(listOfSentences)):\n",
    "        tokens = nltk.word_tokenize(listOfSentences[i])\n",
    "        listOfSentencesInTokens.append(tokens)\n",
    "    return listOfSentencesInTokens\n",
    "\n",
    "\n",
    "listOfTokenizedQuestions = tokenize_sentences(listOfQuestions)\n",
    "for tokenizedItem in listOfTokenizedQuestions[0:5]:\n",
    "      print(\"- \" + str(tokenizedItem))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção das stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho das stop-words: 204\n",
      "Tamanho: 1500\n",
      "- ['cara', 'faixapreta', 'venceu', 'dengue', 'ducatambasco', 'haha', 'bom', 'rever', 'brother']\n",
      "- ['dengue', 'pegou', 'dnv']\n",
      "- ['prefeitura', 'santa', 'cruz', 'divulga', 'plano', 'ação', 'enfrentamento', 'dengue', 'chicungunya', 'zika']\n",
      "- ['seguimos', 'luta', 'contra', 'dengue', 'chikungunya', 'zika', 'vírus', 'três', 'doenças', 'dá']\n",
      "- ['g1', 'pr', 'segunda', 'morte', 'dengue', 'paranaguá', 'confirmada', 'saúde', 'g1']\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords[:10]\n",
    "print(\"Tamanho das stop-words: \" + str(len(stopwords)))\n",
    "\n",
    "def remove_stop_words(listOfTokenizedSentences):\n",
    "    sentencesWithNoStopWords = []\n",
    "    for i in range(len(listOfTokenizedSentences)):\n",
    "        sentenceWithNoStopWord = []\n",
    "        for j in range(len(listOfTokenizedSentences[i])):\n",
    "            if(listOfTokenizedSentences[i][j] not in stopwords):\n",
    "                sentenceWithNoStopWord.append(listOfTokenizedSentences[i][j])\n",
    "        sentencesWithNoStopWords.append(sentenceWithNoStopWord)\n",
    "    return sentencesWithNoStopWords\n",
    "\n",
    "listOfQuestionsWithNoStopWords = remove_stop_words(listOfTokenizedQuestions)\n",
    "print(\"Tamanho: \" + str(len(listOfQuestionsWithNoStopWords)))\n",
    "\n",
    "for itemWithNoStopWord in listOfQuestionsWithNoStopWords[0:5]:\n",
    "    print(\"- \" + str(itemWithNoStopWord))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematização das tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentenças lematizadas e filtradas: \n",
      "Tamanho: 1442\n",
      "- ['car', 'faixapret', 'venc', 'deng', 'ducatambasc', 'hah', 'bom', 'rev', 'broth']\n",
      "- ['deng', 'peg', 'dnv']\n",
      "- ['prefeitur', 'sant', 'cruz', 'divulg', 'plan', 'açã', 'enfrent', 'deng', 'chicunguny', 'zik']\n",
      "- ['segu', 'lut', 'contr', 'deng', 'chikunguny', 'zik', 'vírus', 'três', 'doenc', 'dá']\n",
      "- ['g1', 'pr', 'segund', 'mort', 'deng', 'paranagu', 'confirm', 'saúd', 'g1']\n",
      "...\n",
      "\n",
      "\n",
      "Labels lematizadas e filtradas: \n",
      "Tamanho: 1442\n",
      "doente\n",
      "doente\n",
      "generico\n",
      "generico\n",
      "doente\n",
      "...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"portuguese\")\n",
    "print()\n",
    "\n",
    "def lematizar_tokens(listOfTokenizedSentences):\n",
    "    stemmed_sentences = []\n",
    "    for tokenizedSentence in listOfTokenizedSentences:\n",
    "        stemmed_sentence = []\n",
    "        for token in tokenizedSentence:\n",
    "            stemmed_sentence.append(stemmer.stem(token))\n",
    "        stemmed_sentences.append(stemmed_sentence)\n",
    "    return stemmed_sentences\n",
    "\n",
    "def remove_redundancies(stemmed_sentences, labels):\n",
    "    filtered_list = []\n",
    "    filtered_labels = []\n",
    "    for index in range(len(stemmed_sentences)):\n",
    "        isRedundant = False\n",
    "        for filtered_sentence in filtered_list:\n",
    "            if(filtered_sentence == stemmed_sentences[index]):\n",
    "                isRedundant = True\n",
    "        if(not isRedundant):\n",
    "            filtered_list.append(stemmed_sentences[index])\n",
    "            filtered_labels.append(labels[index])\n",
    "    return {\"features\": filtered_list, \"labels\": filtered_labels}\n",
    "\n",
    "sentencas_lematizadas = lematizar_tokens(listOfQuestionsWithNoStopWords)\n",
    "filtered_dataset = remove_redundancies(sentencas_lematizadas, listOfAnswers)\n",
    "\n",
    "sentencas_lematizadas_e_filtradas = filtered_dataset[\"features\"]\n",
    "labels_lematizadas_e_filtradas = filtered_dataset[\"labels\"]\n",
    "\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(\"Sentenças lematizadas: \")\n",
    "# print(sentencas_lematizadas)\n",
    "# print(\"Tamanho: \" + str(len(sentencas_lematizadas)))\n",
    "# print(\"\\n\")\n",
    "\n",
    "print(\"Sentenças lematizadas e filtradas: \")\n",
    "print(\"Tamanho: \" + str(len(sentencas_lematizadas_e_filtradas)))\n",
    "for stemmedItem in sentencas_lematizadas_e_filtradas[0:5]:\n",
    "    print(\"- \" + str(stemmedItem))\n",
    "print(\"...\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Labels lematizadas e filtradas: \")\n",
    "print(\"Tamanho: \" + str(len(labels_lematizadas_e_filtradas)))\n",
    "for stemmedLabel in labels_lematizadas_e_filtradas[0:5]:\n",
    "    print(stemmedLabel)\n",
    "print(\"...\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construção da bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da bag of words: 3246\n"
     ]
    }
   ],
   "source": [
    "def create_bag_of_words(listOfTokenizedSentences):\n",
    "    wordfreq = {}\n",
    "    for i in range(len(listOfTokenizedSentences)):  #Para cada sentença tokenizada\n",
    "        for token in listOfTokenizedSentences[i]:       # Para cada token em uma sentença tokenizada\n",
    "            if token not in wordfreq.keys():\n",
    "                wordfreq[token] = 1\n",
    "            else:\n",
    "                wordfreq[token] += 1\n",
    "    return wordfreq\n",
    "\n",
    "wordfreq = create_bag_of_words(sentencas_lematizadas_e_filtradas)\n",
    "# print(wordfreq)\n",
    "bag_of_words_size = len(wordfreq.keys())\n",
    "print(\"Tamanho da bag of words: \" + str(bag_of_words_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo do parâmetro IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavras mais frequentes e seus respectivos IDFs: \n",
      "deng: 0.024571260730505327\n",
      "mosquit: 2.026762245684408\n",
      "to: 2.4139739134832223\n",
      "q: 2.629395418703522\n",
      "zik: 2.469765273111638\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "order_degree = ENV_ORDER_DEGREE\n",
    "def get_IDF_wordDictionary(sentencas_lematizadas_e_filtradas, most_freq):\n",
    "    word_idf_values = {}\n",
    "    for token in most_freq:\n",
    "        sentences_containing_word = 0\n",
    "        for tokenized_sentence in sentencas_lematizadas_e_filtradas:\n",
    "            if token in tokenized_sentence:\n",
    "                sentences_containing_word += 1\n",
    "        word_idf_values[token] = np.log(len(sentencas_lematizadas_e_filtradas)/(1 + sentences_containing_word))\n",
    "    return word_idf_values\n",
    "\n",
    "most_freq_dictionary = heapq.nlargest(order_degree , wordfreq, key=wordfreq.get)\n",
    "print(\"Palavras mais frequentes e seus respectivos IDFs: \")\n",
    "idf_wordDictionary = get_IDF_wordDictionary(sentencas_lematizadas_e_filtradas, most_freq_dictionary)\n",
    "for key in list(idf_wordDictionary.keys())[0:5]:\n",
    "    print(key + \": \" + str(idf_wordDictionary[key]))\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculo do parâmetro TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_TF_wordDictionary(sentencas_lematizadas_e_filtradas,most_freq):\n",
    "    word_tf_values = {}\n",
    "    for token in most_freq:\n",
    "        sent_tf_vector = []\n",
    "        for tokenized_sentence in sentencas_lematizadas_e_filtradas:\n",
    "            doc_freq = 0\n",
    "            for word in tokenized_sentence:\n",
    "                if token == word:\n",
    "                    doc_freq += 1\n",
    "            word_tf = doc_freq/len(tokenized_sentence)\n",
    "            sent_tf_vector.append(word_tf)\n",
    "        word_tf_values[token] = sent_tf_vector\n",
    "    return word_tf_values\n",
    "\n",
    "tf_wordDictionary = get_TF_wordDictionary(sentencas_lematizadas_e_filtradas, most_freq_dictionary)\n",
    "# print(\"Dicionário com os valores TF de cada palavra do corpus em cada sentença do dataset\")\n",
    "# for key in list(tf_wordDictionary.keys())[0:5]:\n",
    "#     print(key + \": \" + str(tf_wordDictionary[key]))\n",
    "# print(\"...\")\n",
    "\n",
    "# print(\"Key / Length\")\n",
    "# for key in tf_wordDictionary:\n",
    "#     print(key + \" / \" + str(len(tf_wordDictionary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TF-IDF word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz com os vetores de palavras baseados no modelo TF-IDF\n",
      "(1442, 3000)\n"
     ]
    }
   ],
   "source": [
    "def  get_TF_IDF_matrix(word_idf_values, word_tf_values):\n",
    "    tfidf_values = []\n",
    "    for token in word_tf_values.keys():\n",
    "        tfidf_sentences = []\n",
    "        for tf_sentence in word_tf_values[token]:\n",
    "            tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "            tfidf_sentences.append(tf_idf_score)\n",
    "        tfidf_values.append(tfidf_sentences)\n",
    "    tf_idf_model = np.asarray(tfidf_values)\n",
    "    tf_idf_model = np.transpose(tf_idf_model)\n",
    "    return tf_idf_model\n",
    "\n",
    "tf_model = get_TF_IDF_matrix(idf_wordDictionary, tf_wordDictionary)\n",
    "# print(tf_model)\n",
    "print(\"Matriz com os vetores de palavras baseados no modelo TF-IDF\")\n",
    "print(tf_model.shape)\n",
    "# print(tf_model[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão do dataset nos conjuntos de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de treino:\n",
      "Tamanho: 1299\n",
      "\n",
      "\n",
      "Conjunto de teste:\n",
      "Tamanho: 143\n"
     ]
    }
   ],
   "source": [
    "treino_teste_proportion = ENV_TRAIN_TEST_PROPORTION\n",
    "\n",
    "def count_classes(labels):\n",
    "    classes = []\n",
    "    for index in range(len(labels)):\n",
    "        if(labels[index] not in classes):    \n",
    "            classes.append(labels[index])\n",
    "    return len(classes)\n",
    "\n",
    "def segment_classes(features, labels):\n",
    "    dict = {}\n",
    "    for index in range(len(labels)):\n",
    "        if(labels[index] in dict):\n",
    "            dict[labels[index]].append(features[index])\n",
    "        else:\n",
    "            dict[labels[index]] = [features[index]]\n",
    "    return dict\n",
    "\n",
    "def split_dataset(features, labels, train_proportion):\n",
    "#     print(\"-----------------------------------------------\")\n",
    "#     print(\"Log: Function split_dataset:\")\n",
    "#     print(\"-----------------------------------------------\")\n",
    "\n",
    "    dict_classes = segment_classes(features, labels)\n",
    "#     print(\"dict_classes:\\n\" + \"number of keys: \" + str(len(dict_classes.keys())))\n",
    "#     for key in dict_classes:\n",
    "#         print(\"   key: \" + str(key) + \"\\n\" + \"   value: \" +  str(len(dict_classes[key])) + \" elements in array \\n\")\n",
    "\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    test_features = []\n",
    "    test_labels = []\n",
    "\n",
    "    for key_class in dict_classes:\n",
    "\n",
    "        remove_from_class = math.ceil(len(dict_classes[key_class]) * train_proportion)\n",
    "#         print(\"remove_from_class: \" + str(key_class) + \", \" + str(remove_from_class) + \" elements go to train set\")\n",
    "\n",
    "        for index in range(len(dict_classes[key_class])):\n",
    "            if(index <= remove_from_class -1):\n",
    "                train_features.append(dict_classes[key_class][index])\n",
    "                train_labels.append(key_class)\n",
    "            else:\n",
    "                test_features.append(dict_classes[key_class][index])\n",
    "                test_labels.append(key_class)\n",
    "\n",
    "#     print(\"\\n\")\n",
    "#     print(\"train_features: \" + str(len(train_features)))  \n",
    "#     print(\"train_labels: \" + str(len(train_labels)))\n",
    "#     print(\"test_features: \" + str(len(test_features)))\n",
    "#     print(\"test_labels: \" + str(len(test_labels)))        \n",
    "#     print(\"-----------------------------------------------\")\n",
    "#     print(\"\\n\")\n",
    "    return { \"train\":{\"features\": np.array(train_features), \"labels\": np.array(train_labels)}, \"test\": {\"features\": np.array(test_features), \"labels\": np.array(test_labels)}}\n",
    "\n",
    "# sentence_vectors é uma matriz que contem as sentenças (sem stopwords, lematizadas e filtradas) vetorizadas. Dimensão atual: (81x57)\n",
    "# listOfAnswers corresponde às labels (respostas) correspondentes à cada sentença(pergunta) em sentence_vectors. tamanho atual: (105)\n",
    "# datasets = split_dataset(sentence_vectors, labels_lematizadas_e_filtradas, 0.7)\n",
    "datasets = split_dataset(tf_model, labels_lematizadas_e_filtradas, treino_teste_proportion)\n",
    "\n",
    "dataset_train = datasets[\"train\"] # É um dicionário com o formato {\"features: np.array(), \"labels\": np.array()}\n",
    "dataset_test = datasets[\"test\"] # É um dicionário com o formato {\"features: np.array(), \"labels\": np.array()}\n",
    "\n",
    "#datasets[\"test\"] = np.array(datasets[\"test\"])\n",
    "\n",
    "print(\"Conjunto de treino:\")\n",
    "print(\"Tamanho: \" + str(len(dataset_train[\"features\"])))\n",
    "# print(dataset_train[\"features\"])\n",
    "# for feat in dataset_train[\"features\"]:\n",
    "#   print(feat)\n",
    "# for label in dataset_train[\"labels\"]:\n",
    "#   print(label)\n",
    "print(\"\\n\")\n",
    "print(\"Conjunto de teste:\")\n",
    "print(\"Tamanho: \" + str(len(dataset_test[\"features\"])))\n",
    "# print(dataset_test[\"features\"])\n",
    "# print(dataset_test[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codificação das labels com one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok, a codificação foi a mesma para todos os conjuntos de label. Continuando...\n",
      "\n",
      "\n",
      "Labels de treino codificadas:\n",
      "Tamanho: (1299, 2)\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "...\n",
      "\n",
      "\n",
      "Labels de teste codificadas: \n",
      "Tamanho: (143, 2)\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "def dense_to_one_hot(y_train, num_classes):\n",
    "    y_one_hot = []\n",
    "    encode_output = encode(y_train)\n",
    "    labels_encoded = encode_output[\"labels\"]\n",
    "    encoding_dict = encode_output[\"encoding_dict\"]\n",
    "    for index in range(0,len(labels_encoded)): \n",
    "        nova_label = [0]*num_classes\n",
    "        nova_label[int(labels_encoded[index])]=1\n",
    "        y_one_hot.append(nova_label)\n",
    "    return [np.array(y_one_hot),encoding_dict]\n",
    "\n",
    "# labels é o array com as labels, isto é, os valores indicando se tweets são de pessoas doentes ou não\n",
    "# 1 - pessoa doente\n",
    "# 0 - tweet genérico\n",
    "# A função encode verifica quantas classes diferentes à no array labels. No caso temos 2 classes, 0 e 1.\n",
    "# Para cada uma das classes encontradas, é atribuído um número em ordem crescente.\n",
    "# Para a classe 1 (tweets de pessoas doentes), está sendo atribuído o valor 0\n",
    "# Para a classe 0 (tweets genéricos), está sendo atribuído o valor 1\n",
    "# O valor atribuído a cada classe representa o index da posição no vetor one_hot_encoded que será marcado como 1 para representar a classe\n",
    "# na representação one_hot_encoded.\n",
    "# Isso significa que:\n",
    "# - um tweet de pessoa doente vai receber uma label com a seguinte codificação: [1 0]\n",
    "# - um tweet genérico vai receber uma label com a seguinte codificação: [0 1]\n",
    "def encode(labels):\n",
    "    dict_classes = {}\n",
    "    count = 0\n",
    "    labels_encoded = []\n",
    "    for label in labels:\n",
    "        if label not in dict_classes:\n",
    "            dict_classes[label] = count\n",
    "            count += 1\n",
    "    for label in labels:\n",
    "        labels_encoded.append(dict_classes[label])\n",
    "#     print(\"Dicionário de codificação one_hot, formato: {classe:index}: \" + str(dict_classes))\n",
    "    return {\"labels\":labels_encoded, \"encoding_dict\":dict_classes}\n",
    "\n",
    "def is_encoding_uniform(*dicts):\n",
    "    is_equal = True\n",
    "    dict_ref = dicts[0]\n",
    "    for index in range(1, len(dicts)):\n",
    "        if dicts[index] != dict_ref:\n",
    "            is_equal = False\n",
    "            return is_equal\n",
    "    return is_equal\n",
    "\n",
    "\n",
    "n_classes = count_classes(dataset_test[\"labels\"])\n",
    "[train_labels_one_hot_encoded,encoding_dict1] = dense_to_one_hot(dataset_train[\"labels\"], n_classes)\n",
    "[test_labels_one_hot_encoded,encoding_dict2] = dense_to_one_hot(dataset_test[\"labels\"], n_classes)\n",
    "\n",
    "\n",
    "encoding_dictionary = {}\n",
    "# O código abaixo verifica se os conjuntos de labels passaram pela mesma codificação one_hot\n",
    "if not is_encoding_uniform(encoding_dict1, encoding_dict2):\n",
    "    print(\"ATENÇÃO! Há um problema com a codificação one_hot dos conjuntos de labels. Há conjuntos que receberam codificações diferentes!\")\n",
    "else:\n",
    "    print(\"Ok, a codificação foi a mesma para todos os conjuntos de label. Continuando...\")\n",
    "    print(\"\\n\")\n",
    "    encoding_dictionary = encoding_dict1\n",
    "\n",
    "print(\"Labels de treino codificadas:\")\n",
    "print(\"Tamanho: \" + str(train_labels_one_hot_encoded.shape))\n",
    "print(str(train_labels_one_hot_encoded[0:5]))\n",
    "print('...')\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Labels de teste codificadas: \")\n",
    "print(\"Tamanho: \" + str(test_labels_one_hot_encoded.shape))\n",
    "print(str(test_labels_one_hot_encoded[0:5]))\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=None, n_neighbors=34, p=2,\n",
    "           weights='distance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "scaler.fit(dataset_train[\"features\"])\n",
    "\n",
    "# dataset_train_escalonado = scaler.transform(dataset_train[\"features\"])  \n",
    "# dataset_test_escalonado = scaler.transform(dataset_test[\"features\"]) \n",
    "\n",
    "dataset_train_escalonado = dataset_train[\"features\"] \n",
    "dataset_test_escalonado = dataset_test[\"features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=34, weights='distance')"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(dataset_train_escalonado, train_labels_one_hot_encoded) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo predições sobre o conjunto de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(dataset_test_escalonado)\n",
    "print(y_pred[0:5])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oder_degree: 3000\n",
      "Matrix confusion\n",
      "[[19 26]\n",
      " [ 8 90]]\n",
      "err = 23.776224%\n",
      "acuracy = 76.223776%\n",
      "Accuracy para classe 0: 42.22222222222222.\n",
      "Accuracy para classe 1: 91.83673469387756.\n"
     ]
    }
   ],
   "source": [
    "def from_onehot_to_encoded(labels_onehot):\n",
    "    labels_encoded = []\n",
    "    for element in labels_onehot:\n",
    "        for index in range(len(element)):\n",
    "            if(element[index]==1):\n",
    "                labels_encoded.append(index)\n",
    "    return labels_encoded\n",
    "\n",
    "def matConfusion(y_test, y_pred):\n",
    "    matConf = confusion_matrix(y_test, y_pred)\n",
    "    acc = np.trace(matConf)/np.sum(matConf)\n",
    "    terr  = 1 - acc\n",
    "    return matConf,terr,acc\n",
    "\n",
    "def convert_to_label(encoded_list,encoding_dict):\n",
    "    labels_decoded = []\n",
    "    for encoded_item in encoded_list:\n",
    "        label = get_key(encoded_item, encoding_dict)\n",
    "        labels_decoded.append(label)\n",
    "    return labels_decoded\n",
    "\n",
    "def get_key(val, my_dict): \n",
    "    for key, value in my_dict.items(): \n",
    "            if val == value: \n",
    "                return key \n",
    "    return \"Key does not exist\"\n",
    "\n",
    "def validate(NUM_LABELS, matConf):\n",
    "    acc_classes = np.zeros(NUM_LABELS)\n",
    "    for index in range(NUM_LABELS):\n",
    "        acc_classes[index] = matConf[index,index]/sum(matConf[index,:])\n",
    "    acc_list = []\n",
    "    for acc in acc_classes:\n",
    "        acc_list.append(acc)\n",
    "    return acc_list\n",
    "\n",
    "y_pred_validador = np.argmax(y_pred,1)\n",
    "# print(y_pred_validador[0:5])\n",
    "\n",
    "test_labels_encoded = from_onehot_to_encoded(test_labels_one_hot_encoded)\n",
    "matConf,terr,acc = matConfusion(test_labels_encoded, y_pred_validador)\n",
    "\n",
    "print(\"oder_degree: \" + str(order_degree))\n",
    "#Plotando a matriz de confusão\n",
    "print('Matrix confusion')\n",
    "print(matConf)\n",
    "print('err = %f%s' %(100*terr,'%'))\n",
    "print('acuracy = %f%s' %(100*acc,'%'))\n",
    "\n",
    "# Acurácia para cada classe\n",
    "NUM_LABELS = 2\n",
    "\n",
    "val_data = validate(NUM_LABELS, matConf)\n",
    "\n",
    "c=0\n",
    "for acc in val_data:\n",
    "    print(\"Accuracy para classe \" + str(c) + \": \" + str(acc*100) + \".\")\n",
    "    c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotina de treinamento\n",
    "\n",
    "Para usar essa rotina, rode o seu notebook até construir a bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3245, 1, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3245, 2, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 46.666666666666664.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3245, 3, 0.2447552447552448, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 33.33333333333333.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3245, 4, 0.28671328671328666, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3220, 1, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3220, 2, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 46.666666666666664.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3220, 3, 0.2447552447552448, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 33.33333333333333.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3220, 4, 0.28671328671328666, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3195, 1, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3195, 2, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 46.666666666666664.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3195, 3, 0.2447552447552448, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 33.33333333333333.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3195, 4, 0.28671328671328666, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3170, 1, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3170, 2, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 46.666666666666664.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3170, 3, 0.2447552447552448, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 33.33333333333333.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3170, 4, 0.28671328671328666, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3145, 1, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3145, 2, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 46.666666666666664.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3145, 3, 0.2447552447552448, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 33.33333333333333.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3145, 4, 0.28671328671328666, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3120, 1, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3120, 2, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 46.666666666666664.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3120, 3, 0.2447552447552448, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 33.33333333333333.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3120, 4, 0.28671328671328666, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3095, 1, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3095, 2, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 46.666666666666664.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3095, 3, 0.2447552447552448, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 33.33333333333333.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3095, 4, 0.28671328671328666, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3070, 1, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 22.22222222222222.\n",
      "Accuracy para classe 1: 93.87755102040816.\n",
      "[0.9, 'distance', False, 3070, 2, 0.28671328671328666, 0.9387755102040817, 0, 1]\n",
      "Accuracy para classe 0: 48.888888888888886.\n",
      "Accuracy para classe 1: 85.71428571428571.\n",
      "[0.9, 'distance', False, 3070, 3, 0.25874125874125875, 0.8571428571428571, 0, 1]\n",
      "Accuracy para classe 0: 37.77777777777778.\n",
      "Accuracy para classe 1: 88.77551020408163.\n",
      "[0.9, 'distance', False, 3070, 4, 0.2727272727272727, 0.8877551020408163, 0, 1]\n",
      "Accuracy para classe 0: 97.77777777777777.\n",
      "Accuracy para classe 1: 9.183673469387756.\n",
      "[0.9, 'distance', False, 3045, 1, 0.6293706293706294, 0.09183673469387756, 0, 1]\n",
      "Accuracy para classe 0: 97.77777777777777.\n",
      "Accuracy para classe 1: 9.183673469387756.\n",
      "[0.9, 'distance', False, 3045, 2, 0.6293706293706294, 0.09183673469387756, 0, 1]\n",
      "Accuracy para classe 0: 66.66666666666666.\n",
      "Accuracy para classe 1: 62.244897959183675.\n",
      "[0.9, 'distance', False, 3045, 3, 0.36363636363636365, 0.6224489795918368, 0, 1]\n",
      "Accuracy para classe 0: 66.66666666666666.\n",
      "Accuracy para classe 1: 62.244897959183675.\n",
      "[0.9, 'distance', False, 3045, 4, 0.36363636363636365, 0.6224489795918368, 0, 1]\n",
      "Accuracy para classe 0: 97.77777777777777.\n",
      "Accuracy para classe 1: 8.16326530612245.\n",
      "[0.9, 'distance', False, 3020, 1, 0.6363636363636364, 0.08163265306122448, 0, 1]\n",
      "Accuracy para classe 0: 97.77777777777777.\n",
      "Accuracy para classe 1: 8.16326530612245.\n",
      "[0.9, 'distance', False, 3020, 2, 0.6363636363636364, 0.08163265306122448, 0, 1]\n",
      "Accuracy para classe 0: 100.0.\n",
      "Accuracy para classe 1: 14.285714285714285.\n",
      "[0.9, 'distance', False, 3020, 3, 0.5874125874125874, 0.14285714285714285, 0, 1]\n",
      "Accuracy para classe 0: 100.0.\n",
      "Accuracy para classe 1: 14.285714285714285.\n",
      "[0.9, 'distance', False, 3020, 4, 0.5874125874125874, 0.14285714285714285, 0, 1]\n",
      "Accuracy para classe 0: 97.77777777777777.\n",
      "Accuracy para classe 1: 8.16326530612245.\n",
      "[0.9, 'distance', False, 2995, 1, 0.6363636363636364, 0.08163265306122448, 0, 1]\n",
      "Accuracy para classe 0: 97.77777777777777.\n",
      "Accuracy para classe 1: 8.16326530612245.\n",
      "[0.9, 'distance', False, 2995, 2, 0.6363636363636364, 0.08163265306122448, 0, 1]\n",
      "Accuracy para classe 0: 100.0.\n",
      "Accuracy para classe 1: 14.285714285714285.\n",
      "[0.9, 'distance', False, 2995, 3, 0.5874125874125874, 0.14285714285714285, 0, 1]\n",
      "Accuracy para classe 0: 100.0.\n",
      "Accuracy para classe 1: 14.285714285714285.\n",
      "[0.9, 'distance', False, 2995, 4, 0.5874125874125874, 0.14285714285714285, 0, 1]\n",
      "Accuracy para classe 0: 95.55555555555556.\n",
      "Accuracy para classe 1: 9.183673469387756.\n",
      "[0.9, 'distance', False, 2970, 1, 0.6363636363636364, 0.09183673469387756, 0, 1]\n",
      "Accuracy para classe 0: 95.55555555555556.\n",
      "Accuracy para classe 1: 9.183673469387756.\n",
      "[0.9, 'distance', False, 2970, 2, 0.6363636363636364, 0.09183673469387756, 0, 1]\n",
      "Accuracy para classe 0: 95.55555555555556.\n",
      "Accuracy para classe 1: 16.3265306122449.\n",
      "[0.9, 'distance', False, 2970, 3, 0.5874125874125874, 0.16326530612244897, 0, 1]\n",
      "Accuracy para classe 0: 95.55555555555556.\n",
      "Accuracy para classe 1: 16.3265306122449.\n",
      "[0.9, 'distance', False, 2970, 4, 0.5874125874125874, 0.16326530612244897, 0, 1]\n",
      "Accuracy para classe 0: 95.55555555555556.\n",
      "Accuracy para classe 1: 10.204081632653061.\n",
      "[0.9, 'distance', False, 2945, 1, 0.6293706293706294, 0.10204081632653061, 0, 1]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-904-12794a0a1f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test_escalonado\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Validação\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/neighbors/_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mneigh_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0m_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    660\u001b[0m                 \u001b[0mdelayed_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_tree_query_parallel_helper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 \u001b[0mparallel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prefer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"threads\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m             chunked_results = Parallel(n_jobs, **parallel_kwargs)(\n\u001b[0m\u001b[1;32m    663\u001b[0m                 delayed_query(\n\u001b[1;32m    664\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[0;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "take_from_bag = bag_of_words_size-1\n",
    "escalonar_features = False\n",
    "weights_param = \"distance\"\n",
    "\n",
    "header = [\"train_test_proportion\", \n",
    "          \"weights_param\", \n",
    "          \"escalonar_features\",\n",
    "          \"order_degree\", \n",
    "          \"k_neighbor\", \n",
    "          \"error\", \n",
    "          \"acc\", \n",
    "          \"acc_classe0\", \n",
    "          \"acc_classe1\"\n",
    "         ]\n",
    "\n",
    "while(take_from_bag > 2900):  \n",
    "     \n",
    "    most_freq_dictionary = heapq.nlargest(take_from_bag , wordfreq, key=wordfreq.get)\n",
    "    # IDF\n",
    "    idf_wordDictionary = get_IDF_wordDictionary(sentencas_lematizadas_e_filtradas, most_freq_dictionary)\n",
    "    # TF\n",
    "    tf_wordDictionary = get_TF_wordDictionary(sentencas_lematizadas_e_filtradas, most_freq_dictionary)\n",
    "    # TF-IDF\n",
    "    tf_model = get_TF_IDF_matrix(idf_wordDictionary, tf_wordDictionary)\n",
    "\n",
    "    # Split dataset\n",
    "    datasets = split_dataset(tf_model, labels_lematizadas_e_filtradas, treino_teste_proportion)\n",
    "    dataset_train = datasets[\"train\"] # É um dicionário com o formato {\"features: np.array(), \"labels\": np.array()}\n",
    "    dataset_test = datasets[\"test\"] # É um dicionário com o formato {\"features: np.array(), \"labels\": np.array()}\n",
    "\n",
    "    # One-hot-encoding\n",
    "    n_classes = count_classes(dataset_test[\"labels\"])\n",
    "    [train_labels_one_hot_encoded,encoding_dict1] = dense_to_one_hot(dataset_train[\"labels\"], n_classes)\n",
    "    [test_labels_one_hot_encoded,encoding_dict2] = dense_to_one_hot(dataset_test[\"labels\"], n_classes)\n",
    "    encoding_dictionary = {}\n",
    "    # O código abaixo verifica se os conjuntos de labels passaram pela mesma codificação one_hot\n",
    "    if not is_encoding_uniform(encoding_dict1, encoding_dict2):\n",
    "        print(\"ATENÇÃO! Há um problema com a codificação one_hot dos conjuntos de labels. Há conjuntos que receberam codificações diferentes!\")\n",
    "    else:\n",
    "        encoding_dictionary = encoding_dict1\n",
    "\n",
    "    k_neighbor = 1\n",
    "    while(k_neighbor<5):\n",
    "    \n",
    "        # Build model\n",
    "        classifier = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "        metric_params=None, n_jobs=None, n_neighbors=k_neighbor, p=2,\n",
    "        weights=weights_param)\n",
    "\n",
    "        # Escalonar?\n",
    "        dataset_train_escalonado = []\n",
    "        dataset_test_escalonado = []\n",
    "        if escalonar_features:\n",
    "            dataset_train_escalonado = scaler.transform(dataset_train[\"features\"])  \n",
    "            dataset_test_escalonado = scaler.transform(dataset_test[\"features\"])\n",
    "        else:\n",
    "            dataset_train_escalonado = dataset_train[\"features\"] \n",
    "            dataset_test_escalonado = dataset_test[\"features\"]\n",
    "\n",
    "        # Treino\n",
    "        classifier.fit(dataset_train_escalonado, train_labels_one_hot_encoded) \n",
    "\n",
    "        # Teste\n",
    "        y_pred = classifier.predict(dataset_test_escalonado)\n",
    "\n",
    "        # Validação\n",
    "        y_pred_validador = np.argmax(y_pred,1)\n",
    "        test_labels_encoded = from_onehot_to_encoded(test_labels_one_hot_encoded)\n",
    "        matConf,terr,acc = matConfusion(test_labels_encoded, y_pred_validador)\n",
    "        NUM_LABELS = 2\n",
    "        val_data = validate(NUM_LABELS, matConf)\n",
    "        c=0\n",
    "        acc_classes = []\n",
    "        for acc in val_data:\n",
    "            print(\"Accuracy para classe \" + str(c) + \": \" + str(acc*100) + \".\")\n",
    "            acc_classes.append(c)\n",
    "            c += 1\n",
    "        print([treino_teste_proportion, weights_param, escalonar_features, take_from_bag, k_neighbor, terr, acc, acc_classes[0], acc_classes[1]])\n",
    "            \n",
    "        k_neighbor += 1\n",
    "    take_from_bag -= 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
